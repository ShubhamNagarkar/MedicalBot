{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0791d06a-861d-4df0-845e-d18993e1905d",
   "metadata": {},
   "source": [
    "# RAG-based Medical Bot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ff7c62-eb36-47dc-9128-8a9cd8c9fc4e",
   "metadata": {},
   "source": [
    "##### Any AI based assistance, code completion was not used to solve this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d35748ae-60d8-4e58-93c8-73acfbd71e3e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T15:02:35.967275Z",
     "iopub.status.busy": "2025-04-13T15:02:35.966602Z",
     "iopub.status.idle": "2025-04-13T15:02:58.031502Z",
     "shell.execute_reply": "2025-04-13T15:02:58.030556Z",
     "shell.execute_reply.started": "2025-04-13T15:02:35.967241Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/env/xref/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from utils import read_json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c564c93-5ed5-45c3-9fc0-00f240bce381",
   "metadata": {},
   "source": [
    "# Step 0: Load and Preprocess Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b7e2471-a795-4b1e-9f8f-d84413b08a3b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T15:02:58.033847Z",
     "iopub.status.busy": "2025-04-13T15:02:58.033185Z",
     "iopub.status.idle": "2025-04-13T15:02:58.040190Z",
     "shell.execute_reply": "2025-04-13T15:02:58.039014Z",
     "shell.execute_reply.started": "2025-04-13T15:02:58.033816Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_exact_duplicate_lines(text):\n",
    "    \"\"\"\n",
    "    This function removes the duplicate lines from the passage of text.\n",
    "    \n",
    "    Args:\n",
    "        text (str) : Medical domain answers.\n",
    "        \n",
    "    Returns:\n",
    "        str : De-duplicated text\n",
    "    \"\"\"\n",
    "    lines = [line.strip() for line in text.split('.') if line.strip()]\n",
    "    unique_lines = []\n",
    "    seen_lines = set()\n",
    "    for line in lines:\n",
    "        stripped_line = line.strip() # Consider stripping whitespace\n",
    "        if stripped_line not in seen_lines:\n",
    "            unique_lines.append(line)\n",
    "            seen_lines.add(stripped_line)\n",
    "\n",
    "    return \". \".join(unique_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9d165e41-8da3-438e-ac12-a80247a3a2e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T15:11:37.952707Z",
     "iopub.status.busy": "2025-04-13T15:11:37.952037Z",
     "iopub.status.idle": "2025-04-13T15:11:37.956785Z",
     "shell.execute_reply": "2025-04-13T15:11:37.955927Z",
     "shell.execute_reply.started": "2025-04-13T15:11:37.952677Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Constant Global Variables\n",
    "\n",
    "CHUNK_SIZE = 500\n",
    "CHUNK_OVERLAP = 150\n",
    "CLEAN_DATASET_PATH = './data/clean_questions_to_answers_dataset_v1.json'\n",
    "EMBEDDING_MODEL_ID = \"abhinand/MedEmbed-small-v0.1\"\n",
    "RERANKER_ID =  \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "18efef0f-ddaa-44bf-8bf9-ced18851be11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T15:11:38.172063Z",
     "iopub.status.busy": "2025-04-13T15:11:38.171583Z",
     "iopub.status.idle": "2025-04-13T15:11:38.393377Z",
     "shell.execute_reply": "2025-04-13T15:11:38.392511Z",
     "shell.execute_reply.started": "2025-04-13T15:11:38.172035Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of Questions: 14338\n",
      "Total number of Answers: 14338\n",
      "\n",
      "Q: what is (are) glaucoma?\n",
      "Answer: Glaucoma is a group of diseases that can damage the eye's optic nerve and result in vision loss and blindness. The most common form of the disease is open-angle glaucoma. With early treatment, you can often protect your eyes against serious vision loss. (Watch the video to learn more about glaucoma. To enlarge the video, click the brackets in the lower right-hand corner. To reduce the video, press the Escape (Esc) button on your keyboard. ) See this graphic for a quick overview of glaucoma, including how many people it affects, whos at risk, what to do if you have it, and how to learn more. See a glossary of glaucoma terms. The optic nerve is a bundle of more than 1 million nerve fibers. It connects the retina to the brain. Open-angle glaucoma is the most common form of glaucoma. In the normal eye, the clear fluid leaves the anterior chamber at the open angle where the cornea and iris meet. When the fluid reaches the angle, it flows through a spongy meshwork, like a drain, and leaves the eye. Sometimes, when the fluid reaches the angle, it passes too slowly through the meshwork drain, causing the pressure inside the eye to build. If the pressure damages the optic nerve, open-angle glaucoma -and vision loss -may result. National Eye Institute National Institutes of Health 2020 Vision Place Bethesda, MD 20892-3655 301-496-5248 E-mail: 2020@nei. nih. gov The Glaucoma Foundation 80 Maiden Lane, Suite 700 New York, NY 10038 212-285-0080 Glaucoma Research Foundation 251 Post Street, Suite 600 San Francisco, CA 94108 1-800-826-6693 Glaucoma is a group of diseases that can damage the eye's optic nerve. It is a leading cause of blindness in the United States. It usually happens when the fluid pressure inside the eyes slowly rises, damaging the optic nerve. Often there are no symptoms at first. Without treatment, people with glaucoma will slowly lose their peripheral, or side vision. They seem to be looking through a tunnel. Over time, straight-ahead vision may decrease until no vision remains. A comprehensive eye exam can tell if you have glaucoma. People at risk should get eye exams at least every two years. They include African Americans over age 40 People over age 60, especially Mexican Americans People with a family history of glaucoma There is no cure, but glaucoma can usually be controlled. Early treatment can help protect your eyes against vision loss. Treatments usually include prescription eyedrops and/or surgery. NIH: National Eye Institute\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "dataset = read_json(CLEAN_DATASET_PATH)\n",
    "\n",
    "context_passages = []\n",
    "dataset_questions = []\n",
    "\n",
    "for question, answers in dataset.items():\n",
    "    combined_passage = \" \".join(answers)    \n",
    "    context_passages.append(remove_exact_duplicate_lines(combined_passage)) # de-duplicate the data\n",
    "    dataset_questions.append(question)\n",
    "\n",
    "print(f'Total number of Questions: {len(dataset_questions)}')\n",
    "print(f'Total number of Answers: {len(context_passages)}')\n",
    "\n",
    "# Processed Dataset Preview\n",
    "for q, a_list in list(zip(dataset_questions, context_passages))[:1]:\n",
    "    print(f\"\\nQ: {q}\\nAnswer: {a_list}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a724825-d9d6-4bbb-983c-b55b9f411e1e",
   "metadata": {},
   "source": [
    "# Step 1: Create Data Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "005126c3-1c45-4c02-ae86-bb5e193c2906",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T15:11:38.518732Z",
     "iopub.status.busy": "2025-04-13T15:11:38.518168Z",
     "iopub.status.idle": "2025-04-13T15:11:42.875393Z",
     "shell.execute_reply": "2025-04-13T15:11:42.874496Z",
     "shell.execute_reply.started": "2025-04-13T15:11:38.518701Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of chunks created: 58741\n"
     ]
    }
   ],
   "source": [
    "# Splits long passages into chunks of CHUNK_SIZE = 300 characters with an overlap of 100 characters.\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    "    length_function=len,\n",
    ")\n",
    "\n",
    "all_chunks = []\n",
    "chunk_to_question_map = {}\n",
    "\n",
    "for i, doc in enumerate(context_passages):\n",
    "    chunks = text_splitter.split_text(doc)\n",
    "    question = dataset_questions[i]\n",
    "    for chunk in chunks:\n",
    "        chunk_to_question_map[chunk]=question\n",
    "        all_chunks.append({\n",
    "            \"text\": chunk,\n",
    "            \"question\": question,  \n",
    "            \"answer_index\": i + 1\n",
    "        })\n",
    "\n",
    "print(f\"Total number of chunks created: {len(all_chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "be15daeb-33e5-4251-828a-0045c2b07a3f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T15:11:42.877554Z",
     "iopub.status.busy": "2025-04-13T15:11:42.876993Z",
     "iopub.status.idle": "2025-04-13T15:11:42.887899Z",
     "shell.execute_reply": "2025-04-13T15:11:42.886980Z",
     "shell.execute_reply.started": "2025-04-13T15:11:42.877523Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of questions: 14338\n"
     ]
    }
   ],
   "source": [
    "question_metadata = []\n",
    "\n",
    "for i,quest in enumerate(dataset_questions):\n",
    "    question_metadata.append({\n",
    "        \"text\": quest,\n",
    "        \"answer_index\": i + 1\n",
    "    }) \n",
    "\n",
    "print(f\"Total number of questions: {len(question_metadata)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e2606b-295e-4f81-a2ae-dab3f3a62d0e",
   "metadata": {},
   "source": [
    "# Step 2: Create Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7bdbea13-273c-4d10-a7ec-d06d3a75a78c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T15:11:42.889478Z",
     "iopub.status.busy": "2025-04-13T15:11:42.888906Z",
     "iopub.status.idle": "2025-04-13T15:13:02.811154Z",
     "shell.execute_reply": "2025-04-13T15:13:02.810261Z",
     "shell.execute_reply.started": "2025-04-13T15:11:42.889434Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 58741 answer embeddings of dimension 384\n",
      "Generated 14338 question embeddings of dimension 384\n"
     ]
    }
   ],
   "source": [
    "# BGE model fine-tuned on medical domain data.\n",
    "encoder_model = SentenceTransformer(EMBEDDING_MODEL_ID)\n",
    "\n",
    "chunk_texts = [chunk_info['text'] for chunk_info in all_chunks]\n",
    "\n",
    "# converts text to embeddings.\n",
    "answer_embeddings = encoder_model.encode(chunk_texts)\n",
    "question_embeddings = encoder_model.encode(dataset_questions)\n",
    "\n",
    "embedding_dimension = answer_embeddings.shape[1]\n",
    "num_chunks = len(all_chunks)\n",
    "\n",
    "print(f\"Generated {num_chunks} answer embeddings of dimension {embedding_dimension}\")\n",
    "print(f\"Generated {len(dataset_questions)} question embeddings of dimension {question_embeddings.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd1fdf1-6d97-4add-8dd4-0563b54acf4a",
   "metadata": {},
   "source": [
    "# Step 3: Create Vector Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e19eada6-a024-475f-8068-c60c07f1cf3f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T15:13:02.813746Z",
     "iopub.status.busy": "2025-04-13T15:13:02.813143Z",
     "iopub.status.idle": "2025-04-13T15:13:02.927739Z",
     "shell.execute_reply": "2025-04-13T15:13:02.926406Z",
     "shell.execute_reply.started": "2025-04-13T15:13:02.813715Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creating vector index for all the answers\n",
    "\n",
    "answer_index = faiss.IndexFlatL2(embedding_dimension)\n",
    "faiss.normalize_L2(answer_embeddings)\n",
    "answer_index.add(answer_embeddings)\n",
    "metadata_store = all_chunks\n",
    "\n",
    "\n",
    "# Creating vector index for all the questions\n",
    "\n",
    "question_index = faiss.IndexFlatL2(embedding_dimension)\n",
    "faiss.normalize_L2(question_embeddings)\n",
    "question_index.add(question_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a13eaed-19c3-4b63-990e-3b968e0f73ca",
   "metadata": {},
   "source": [
    "# Step 4: Initialize Reranker and Smart Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4b0eb252-6541-41bd-93b6-c00da22b1209",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T15:13:02.929829Z",
     "iopub.status.busy": "2025-04-13T15:13:02.929539Z",
     "iopub.status.idle": "2025-04-13T15:13:02.939561Z",
     "shell.execute_reply": "2025-04-13T15:13:02.938436Z",
     "shell.execute_reply.started": "2025-04-13T15:13:02.929800Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def smart_hybrid_rerank(\n",
    "    user_query: str,\n",
    "    query_encoder,\n",
    "    answer_index,\n",
    "    question_index,\n",
    "    chunk_passages: list,\n",
    "    chunk_to_question_map: list,\n",
    "    dataset_questions: list,\n",
    "    reranker,\n",
    "    top_k=50,\n",
    "    rank_k=10,\n",
    "    final_k=3,\n",
    "):\n",
    "    \"\"\"\n",
    "    This function encapsulates the re-ranking logic for a given query and topk retrieved chunks of answers.\n",
    "    It reranks the retrieved results based on:\n",
    "        - similarity between user query and retrieved chunk's question.\n",
    "        - similarity between user query and questions retrieved from questions index.\n",
    "        - similarity between user query and retrieved chunk.\n",
    "    If the average score of retrieved and ranked documents is all negative and below a certain threshold, then it assigns an out of domain flag\n",
    "    to the query (Query not related to the database).\n",
    "    \n",
    "    \"\"\"\n",
    "    # encode user query\n",
    "    query_embedding = query_encoder.encode([user_query])\n",
    "    faiss.normalize_L2(query_embedding)\n",
    "    \n",
    "    # search answer index\n",
    "    answer_distances, chunk_indices = answer_index.search(query_embedding, top_k)\n",
    "    \n",
    "    # retrieve chunks\n",
    "    retrieved_chunks = [chunk_passages[i]['text'] for i in chunk_indices[0]]\n",
    "\n",
    "    # retrieve questions corresponding to the retrieved chunks\n",
    "    retrieved_questions = [chunk_to_question_map[chunk] for chunk in retrieved_chunks]\n",
    "    \n",
    "    # search question index\n",
    "    question_distances, question_indices = question_index.search(query_embedding, 2) # only top 2\n",
    "    similar_questions = [dataset_questions[i] for i in question_indices[0]]\n",
    "\n",
    "    # score 1: sim(user query -- retrieved chunks)\n",
    "    match_scores = reranker.predict([(user_query, chunk) for chunk in retrieved_chunks]) \n",
    "    \n",
    "    # score 2: sim(user query -- retrieved chunks' questions)\n",
    "    base_scores = reranker.predict([(user_query, q) for q in retrieved_questions])  \n",
    "    \n",
    "    # score 3: sim(retrieved chunks' questions -- similar questions from question index (only top 2))\n",
    "    sim_scores = [\n",
    "        max(reranker.predict([(rq, sq) for sq in similar_questions]))\n",
    "        for rq in retrieved_questions\n",
    "    ]\n",
    "    \n",
    "    # weighted average\n",
    "    final_scores = [0.3 * ms + 0.6 * qs + 0.1*ss for ms, qs, ss in zip(match_scores, base_scores, sim_scores)]\n",
    "    \n",
    "    # if mean of all scores is negative and below -0.4 threshold, it means the query is out of domain scope.\n",
    "    avg = np.mean(final_scores)\n",
    "    if avg < -0.4:\n",
    "        score_threshold = 0\n",
    "    else:\n",
    "        score_threshold = avg + 0.05\n",
    "\n",
    "    filtered = [\n",
    "        (score, passage, q)\n",
    "        for score, passage, q in zip(final_scores, retrieved_chunks, retrieved_questions)\n",
    "        if score >= score_threshold\n",
    "    ]\n",
    "\n",
    "    if not filtered:\n",
    "        fallback = [\n",
    "            (score, passage, q)\n",
    "            for score, passage, q in zip(final_scores, retrieved_chunks, retrieved_questions)\n",
    "        ]\n",
    "        return fallback[:1], True\n",
    "    else:\n",
    "        return sorted(filtered, reverse=True)[:rank_k], False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3e5ee0b0-bfdf-4c30-80fe-1579c59f1ee0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T15:13:02.941481Z",
     "iopub.status.busy": "2025-04-13T15:13:02.941025Z",
     "iopub.status.idle": "2025-04-13T15:13:03.455398Z",
     "shell.execute_reply": "2025-04-13T15:13:03.454364Z",
     "shell.execute_reply.started": "2025-04-13T15:13:02.941434Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are trying to use a model that was created with Sentence Transformers version 4.1.0.dev0, but you're currently using version 4.0.2. This might cause unexpected behavior or errors. In that case, try to update to the latest version.\n"
     ]
    }
   ],
   "source": [
    "reranker = CrossEncoder(RERANKER_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e91104-6d0e-4819-a17c-5c2a381aba94",
   "metadata": {},
   "source": [
    "# Step 5: Load Open-source LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "9e21aaea-55a6-43f7-9628-bde9196ac24f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T16:43:04.379829Z",
     "iopub.status.busy": "2025-04-13T16:43:04.379066Z",
     "iopub.status.idle": "2025-04-13T16:43:05.623269Z",
     "shell.execute_reply": "2025-04-13T16:43:05.622182Z",
     "shell.execute_reply.started": "2025-04-13T16:43:04.379791Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name ='TinyLlama/TinyLlama-1.1B-Chat-v1.0' # small LLM based on GPU and memory constraints.\n",
    "\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=\"auto\")\n",
    "llm_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "llm_pipe = pipeline(\"text-generation\", model=llm_model, tokenizer=llm_tokenizer, max_new_tokens=400, do_sample=False, return_full_text=False, repetition_penalty=1)\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=llm_pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6a5b61-783f-469d-a18d-b93298a43d1d",
   "metadata": {},
   "source": [
    "# Step 6: LangChain Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "01cf6ebb-268e-45e2-9b1b-e262be2ee013",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T16:43:05.625258Z",
     "iopub.status.busy": "2025-04-13T16:43:05.624984Z",
     "iopub.status.idle": "2025-04-13T16:43:05.629516Z",
     "shell.execute_reply": "2025-04-13T16:43:05.628509Z",
     "shell.execute_reply.started": "2025-04-13T16:43:05.625229Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "You are a knowledgeable and concise medical assistant.\n",
    "Given the context below, answer the user's question **in your own words**.\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Provide a clear, factual, and summarized response. Do **not** repeat the text verbatim.\n",
    "\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "30d4aa68-cc9b-4951-a588-2f906e2bf628",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T16:43:05.923219Z",
     "iopub.status.busy": "2025-04-13T16:43:05.922666Z",
     "iopub.status.idle": "2025-04-13T16:43:05.935040Z",
     "shell.execute_reply": "2025-04-13T16:43:05.934029Z",
     "shell.execute_reply.started": "2025-04-13T16:43:05.923191Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(input_variables=[\"question\", \"context\"], template=template)\n",
    "chain = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d98525-24da-4d41-b12a-29d5520bda5d",
   "metadata": {},
   "source": [
    "# Step 7: ChatBot Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "5082e69d-3bfa-44e0-9eb9-ebdfa7dadc0c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T16:43:06.526448Z",
     "iopub.status.busy": "2025-04-13T16:43:06.525882Z",
     "iopub.status.idle": "2025-04-13T16:43:06.530907Z",
     "shell.execute_reply": "2025-04-13T16:43:06.529900Z",
     "shell.execute_reply.started": "2025-04-13T16:43:06.526419Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "torch.backends.cuda.enable_flash_sdp(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "856c11d5-6ccb-496d-960f-89150ead7a24",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T16:45:09.994739Z",
     "iopub.status.busy": "2025-04-13T16:45:09.994060Z",
     "iopub.status.idle": "2025-04-13T16:45:10.000312Z",
     "shell.execute_reply": "2025-04-13T16:45:09.999415Z",
     "shell.execute_reply.started": "2025-04-13T16:45:09.994707Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_llm_response(query):\n",
    "    \"\"\"\n",
    "    This function extracts LLMs response based on a single query input.\n",
    "    \"\"\"\n",
    "    top_chunks, is_ood = smart_hybrid_rerank(\n",
    "        user_query=query,\n",
    "        query_encoder=encoder_model,\n",
    "        answer_index=answer_index,\n",
    "        question_index=question_index,\n",
    "        chunk_passages=all_chunks,\n",
    "        chunk_to_question_map=chunk_to_question_map,\n",
    "        dataset_questions=dataset_questions,\n",
    "        reranker=reranker\n",
    "    )\n",
    "\n",
    "    context = \" \".join([chunk for _, chunk, _ in top_chunks[:10]]) # put top 10 chunks in the context window.\n",
    "    response = chain.run({\"question\": query, \"context\": context})\n",
    "\n",
    "    if is_ood: # out of domain query?\n",
    "        return \"This may be an out-of-scope query.\"\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61581b48-ab02-4ec0-bef3-960dafecbabf",
   "metadata": {},
   "source": [
    "### Example responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "3ab80e17-1564-439a-9bce-6e294ac68f57",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T16:44:16.838832Z",
     "iopub.status.busy": "2025-04-13T16:44:16.838284Z",
     "iopub.status.idle": "2025-04-13T16:44:21.449585Z",
     "shell.execute_reply": "2025-04-13T16:44:21.448630Z",
     "shell.execute_reply.started": "2025-04-13T16:44:16.838802Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This may be an out-of-scope query.'"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unrelated question\n",
    "query = \"What is a Transformer based model?\"\n",
    "get_llm_response(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "a0d8f079-b512-4620-8cc2-76a97b52a420",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T16:45:17.145823Z",
     "iopub.status.busy": "2025-04-13T16:45:17.145136Z",
     "iopub.status.idle": "2025-04-13T16:45:32.706035Z",
     "shell.execute_reply": "2025-04-13T16:45:32.705142Z",
     "shell.execute_reply.started": "2025-04-13T16:45:17.145791Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nGlaucoma is a group of eye disorders in which the optic nerves connecting the eyes and the brain are progressively damaged. This damage can lead to reduction in side (peripheral) vision and eventual blindness. Other signs and symptoms may include bulging eyes, excessive tearing, and abnormal sensitivity to light (photophobia). The term \"early-onset glaucoma\" may be used when the disorder appears before the age of 40. In most people with glaucoma, the damage to the optic nerves is caused by increased pressure within the eyes (intraocular pressure). Intraocular pressure depends on a balance between fluid entering and leaving the eyes. Usually glaucoma develops in older adults, in whom the risk of developing the disorder may be affected by a variety of medical conditions including high blood pressure (hypertension) and diabetes mellitus, as well as family history. The risk of early-onset glaucoma depends mainly on heredity. Structural abnormalities that impede fluid drainage in the eye may be present at birth and usually become apparent during the first year of life. Such abnormalities may be part of a genetic disorder that affects many body systems, called a syndrome. If glaucoma appears before the age of 5 without other associated abnormalities, it is called primary congenital glaucoma. Other individuals experience early onset of primary open-angle glaucoma, the most common adult form of glaucoma. If primary open-angle glaucoma develops during childhood or early adulthood, it is called juvenile open-angle glaucoma. Glaucoma\" may be used when the disorder appears before the age of 4'"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simple question\n",
    "query = \"What is Glaucoma?\"\n",
    "get_llm_response(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "09534ee1-99b5-4fba-b1e6-a3dce4df7512",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T16:46:05.762597Z",
     "iopub.status.busy": "2025-04-13T16:46:05.761782Z",
     "iopub.status.idle": "2025-04-13T16:46:20.639060Z",
     "shell.execute_reply": "2025-04-13T16:46:20.638154Z",
     "shell.execute_reply.started": "2025-04-13T16:46:05.762566Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'High blood pressure is a common disease in which blood flows through blood vessels (arteries) at higher than normal pressures. It is a medical condition that can lead to various health problems, including heart disease, stroke, and kidney disease. Treatment for high blood pressure involves lifestyle changes and medications to control blood pressure. Health care providers develop treatment plans based on the diagnosis, lifestyle changes, and medicines that work best for each individual. Treatment plans may evolve until blood pressure control is achieved. In most cases, the goal is to keep blood pressure below 140/90 mmHg (130/80 if you have diabetes or chronic kidney disease). Normal blood pressure is less than 120/80. Ask your doctor what your blood pressure goal should be. If you have high blood pressure, you will need to treat it and control it for life. This means making lifestyle changes, and, in some cases, taking prescribed medicines, and getting ongoing and maintain normal blood pressure readings. Healthy Eating To help treat high blood pressure, health care providers recommend that you limit sodium and salt intake, increase potassium, and eat foods that are heart healthy. Limiting Sodium and Salt A low-sodium diet can help you manage your blood pressure. You should try to limit the amount of sodium that you eat. This means choosing and preparing foods that are lower in salt and sodium. Try to use low-sodium and no added salt foods and plan, keep up your healthy lifestyle habits. The combination of the medicines and the healthy lifestyle habits helps control and lower your high blood pressure. Some people develop resistant or uncontrolled high blood pressure. This can happen when the medications they are taking do'"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Asking two questions as one question.\n",
    "query = \"What is High Blood pressure and how to treat it?\"\n",
    "get_llm_response(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01377778-b09f-4b27-92b4-418aad16ccf6",
   "metadata": {},
   "source": [
    "### Comments:\n",
    "\n",
    "The length of the responses can be controlled by max_tokens, also if the repetition_penalty is increased, the answers are much shorter and concise. The output here is more grounded based on retrieved documents. If temperature is increased, it will be less verbatim to the documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf300145-c796-4b85-ab33-43ec23c8c97d",
   "metadata": {},
   "source": [
    "# Chatbot Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "42468292-011a-4571-97aa-e1e391921fc2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T16:04:48.704089Z",
     "iopub.status.busy": "2025-04-13T16:04:48.703346Z",
     "iopub.status.idle": "2025-04-13T16:04:48.707917Z",
     "shell.execute_reply": "2025-04-13T16:04:48.707009Z",
     "shell.execute_reply.started": "2025-04-13T16:04:48.704057Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import random\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a27a437e-0826-49d8-a8cf-0fd5898f91ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T15:16:58.629160Z",
     "iopub.status.busy": "2025-04-13T15:16:58.628589Z",
     "iopub.status.idle": "2025-04-13T15:17:00.662539Z",
     "shell.execute_reply": "2025-04-13T15:17:00.661610Z",
     "shell.execute_reply.started": "2025-04-13T15:16:58.629127Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "rouge = evaluate.load(\"rouge\")\n",
    "bertscore = evaluate.load(\"bertscore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "72ba8d75-33bc-49b2-b805-18e97b4ffd41",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T16:04:50.642437Z",
     "iopub.status.busy": "2025-04-13T16:04:50.641847Z",
     "iopub.status.idle": "2025-04-13T16:04:50.647642Z",
     "shell.execute_reply": "2025-04-13T16:04:50.646725Z",
     "shell.execute_reply.started": "2025-04-13T16:04:50.642408Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_llm_input(queries):\n",
    "    \n",
    "    inputs = []\n",
    "    \n",
    "    for query in tqdm.tqdm(queries):\n",
    "        top_chunks, is_ood = smart_hybrid_rerank(\n",
    "            user_query=query,\n",
    "            query_encoder=encoder_model,\n",
    "            answer_index=answer_index,\n",
    "            question_index=question_index,\n",
    "            chunk_passages=all_chunks,\n",
    "            chunk_to_question_map=chunk_to_question_map,\n",
    "            dataset_questions=dataset_questions,\n",
    "            reranker=reranker\n",
    "        )\n",
    "    \n",
    "        context = \" \".join([chunk for _, chunk, _ in top_chunks[:10]])\n",
    "    \n",
    "        inputs.append({\n",
    "            \"question\": query,\n",
    "            \"context\": context\n",
    "        })\n",
    "    \n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "e6a55922-337a-4bf3-9dd3-7f0a15bc6439",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T16:04:51.595068Z",
     "iopub.status.busy": "2025-04-13T16:04:51.594472Z",
     "iopub.status.idle": "2025-04-13T16:04:51.600307Z",
     "shell.execute_reply": "2025-04-13T16:04:51.599310Z",
     "shell.execute_reply.started": "2025-04-13T16:04:51.595036Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_set = 50\n",
    "\n",
    "indices = list(range(len(dataset_questions)))\n",
    "\n",
    "selected_indices = random.sample(indices, sample_set)\n",
    "\n",
    "# Random test set for LLM evaluation\n",
    "\n",
    "test_questions = [dataset_questions[i] for i in selected_indices]\n",
    "test_passages = [context_passages[i] for i in selected_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "5670cc8b-10a4-4077-aaa0-b65947eebc89",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T16:04:51.798609Z",
     "iopub.status.busy": "2025-04-13T16:04:51.798163Z",
     "iopub.status.idle": "2025-04-13T16:04:51.804057Z",
     "shell.execute_reply": "2025-04-13T16:04:51.803127Z",
     "shell.execute_reply.started": "2025-04-13T16:04:51.798581Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 50)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_questions), len(test_passages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "9b9a82a6-8637-44d2-906e-fedea1320cb9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T16:04:52.026228Z",
     "iopub.status.busy": "2025-04-13T16:04:52.025556Z",
     "iopub.status.idle": "2025-04-13T16:05:15.491669Z",
     "shell.execute_reply": "2025-04-13T16:05:15.490746Z",
     "shell.execute_reply.started": "2025-04-13T16:04:52.026191Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:23<00:00,  2.13it/s]\n"
     ]
    }
   ],
   "source": [
    "llm_inputs = get_llm_input(test_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "0f4013e9-5c44-46db-be5d-c625cb32506c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T16:07:01.903501Z",
     "iopub.status.busy": "2025-04-13T16:07:01.902401Z",
     "iopub.status.idle": "2025-04-13T16:17:10.228343Z",
     "shell.execute_reply": "2025-04-13T16:17:10.227376Z",
     "shell.execute_reply.started": "2025-04-13T16:07:01.903419Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating answers: 100%|██████████| 50/50 [10:08<00:00, 12.17s/it]\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "for input_dict in tqdm.tqdm(llm_inputs, desc=\"Generating answers\"):\n",
    "    result = chain.apply([input_dict])[0] \n",
    "    predictions.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "bf3a8a72-794e-47aa-8277-910ff347a7ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T16:21:01.243320Z",
     "iopub.status.busy": "2025-04-13T16:21:01.242507Z",
     "iopub.status.idle": "2025-04-13T16:21:01.247486Z",
     "shell.execute_reply": "2025-04-13T16:21:01.246542Z",
     "shell.execute_reply.started": "2025-04-13T16:21:01.243289Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm_answers = [pred['text'] for pred in predictions]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97f8037-d937-4b57-91f6-ad0bd58acb16",
   "metadata": {},
   "source": [
    "# ROGUE Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "9b8291c8-c47f-455f-8056-3a257f8795de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T16:21:11.308044Z",
     "iopub.status.busy": "2025-04-13T16:21:11.307173Z",
     "iopub.status.idle": "2025-04-13T16:21:13.475972Z",
     "shell.execute_reply": "2025-04-13T16:21:13.475026Z",
     "shell.execute_reply.started": "2025-04-13T16:21:11.308011Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': 0.4115395382943542,\n",
       " 'rouge2': 0.28478724331104943,\n",
       " 'rougeL': 0.3245266069281287,\n",
       " 'rougeLsum': 0.32423982991848277}"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge.compute(predictions=llm_answers, references=test_passages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eeab843-a6ae-43ca-b5c5-e38ad404b9f5",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "#### ROUGE Metrics (Lexical Overlap)\n",
    "ROUGE-1 = 0.41 basically it shows that 41% of the unigrams in the reference answers were also present in the generated answers.\n",
    "\n",
    "ROUGE-2 = 0.28 indicates that the generated answers maintain some sort of fluency and phrase similarity, but not perfect replication.\n",
    "\n",
    "ROUGE-L = 0.325 implies moderate structural alignment, which means the model often reorders or paraphrases content rather than copying exact sequences.\n",
    "\n",
    "#### Conclusion:\n",
    "Based on these results and some manual inspection, it was evident that the model's output has certain level of lexical and structural overlap with the reference passages. The important words were replicated from the provided documents. The model didn't hallucinate much and was grounded by the retrieved passages. There is evidently some paraphrasing and hence exact matches of the phrases from the passages is less. The paraphrasing can be controlled by the temperature of the LLM but those experiments are for future scope based on broader requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a2f2ec-b07b-4f5d-8164-5c505e455a45",
   "metadata": {},
   "source": [
    "# Bert Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "42eb17ae-b164-481e-a145-7aff2724784e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T16:21:23.355533Z",
     "iopub.status.busy": "2025-04-13T16:21:23.354825Z",
     "iopub.status.idle": "2025-04-13T16:21:25.464474Z",
     "shell.execute_reply": "2025-04-13T16:21:25.463467Z",
     "shell.execute_reply.started": "2025-04-13T16:21:23.355501Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTScore:\n",
      "Precision: 0.8591\n",
      "Recall:    0.8734\n",
      "F1:        0.8656\n"
     ]
    }
   ],
   "source": [
    "bertscore_results = bertscore.compute(predictions=llm_answers, references=test_passages, lang=\"en\")\n",
    "print(\"BERTScore:\")\n",
    "print(f\"Precision: {np.mean(bertscore_results['precision']):.4f}\")\n",
    "print(f\"Recall:    {np.mean(bertscore_results['recall']):.4f}\")\n",
    "print(f\"F1:        {np.mean(bertscore_results['f1']):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d6fc5f-e757-4c43-8734-b91365e09e83",
   "metadata": {},
   "source": [
    "#### BERTScore Metrics (Semantic Similarity)\n",
    "F1 = 0.866 shows that the generated answers are semantically very close to the reference answers — even when exact words differ.\n",
    "\n",
    "Precision = 0.859 indicates the model mostly avoids hallucinating extra or irrelevant information.\n",
    "\n",
    "Recall = 0.873 shows that the model includes a large portion of the relevant content from the reference.\n",
    "\n",
    "#### Conclusion:\n",
    "The LLM definitely understands and expresses the correct medical facts from the supplied passages. High recall implies that answers are informative and complete, while high precision shows they’re on-topic and relevant. \n",
    "\n",
    "Apart from these metrics, based on manual inspection, its clear that the RAG system produces factually correct and relevant answers, with minor differences in phrasing or answer structure. The model often paraphrases or reformats the information instead of copying exact sentences — which is the desired behavior in to make the bot a good assistant. While lexical similarity (ROUGE) is moderate, semantic similarity (BERTScore) is high, indicating that the overall solution approach performs well for Medical Q&A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3a8896-0cb9-4450-9b56-3d9ddaa0bc84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c86f69ce-d215-4a89-88da-4d266cd5ad02",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Future Scope:\n",
    "\n",
    "The current outline solutions works well for the Small Memory and Single GPU constraints. There are various ways to improve the performane of each of these components of the RAG architecture.\n",
    "\n",
    "### Embedding Model:\n",
    "- Larger Models trained on specific Medical Domain can be used to encode user queries and documents.\n",
    "- These model embeddings will be more robust and will focus on medical terminologies more than regular english language and expressions.\n",
    "- The models used in this solution have lesser parameters and the embedding dimension is extremely small (384). Can go for larger model size with larger embedding dimension.\n",
    "\n",
    "### RAG:\n",
    "\n",
    "#### Data Filtering before Indexing\n",
    "- An LLM can be used to filter out unnecessary answers that do not provide any contextual information related to the question. This reduces the number of documents indexed into the vector database, making the index more clean, and full of relevant content. This in turn helps to retrieve semantically similar and useful documents.\n",
    "\n",
    "#### Topic Modeling/User Intent Classification\n",
    "- Another classifier can be trained to classfiy the user query into certain topics. For example, in this dataset, there are various topics based on health conditions ~ Glaucoma, Blood Pressure, etc. There are several questions per topic. If a classifier predicts what topic the user query belongs to, that information can be used to either semantically retrieve documents only from specific topic (reducing unncessary noise) or can be used in re-ranking step to rank documents based on the topics.\n",
    "- Can use this topic to understand if the query is out of domain or in domain.\n",
    "\n",
    "#### Query rewriting\n",
    "- LLMs can be used to expand the user query for better retrieval results. Add more domain knowledge to the user query if its very short.\n",
    "- Augment the query with user history.\n",
    "\n",
    "#### Indexing & Retrieval\n",
    "- There are various large scale efficient Vector indexes to explore (Milvus, DBX, PineCone,etc.)\n",
    "- Explore Contextual RAG ~ Use LLMs to generate context about each document chunk, and prepend that context to the respective chunk before indexing. This improves the retrieval accuracy.\n",
    "- Use hybrid search instead of just cosine similarity to retrieve similar documents. BM25 and cosine similarity helps to retrieve better results.\n",
    "- Use LLM to improve the ranking/ordering of the retrieved chunks. Can construct a prompt to verify if the user query and the document matches.\n",
    "- Agentic RAG if compute and latency isn't too much of an issue.\n",
    "\n",
    "#### LLM for Chatbot\n",
    "- Explore LLMs with larger context window. There are several Medical LLMs fine-tuned on medical data that can be used for the final step in this architecture.\n",
    "- LLMs with atleast 7B-8B parameters would definitely improve the performance ~ Rogue and Bert Score.\n",
    "- If there is a well curated dataset on medical conditions and general medical information, fine-tuning using LoRA can be another option to ground the responses with respect to medical domain.\n",
    "- Use LLMs as Judge to make sure if the generated answer and user query match.\n",
    "\n",
    "#### Feasibility\n",
    "- So many LLM calls cannot be incorporated in a real-time system if the latency is an issue. \n",
    "- Hosting multiple LLMs is expensive, multiple LLM calls involves lot of GPU compute and dedicated deployment server.\n",
    "- As the documents scale, larger vector index is required which is hosted 24*7.\n",
    "- LoRA Fine-tuning  OpenSource models can be cheaper alternative than using paid APIs based on user traffic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01f216a-19b7-45bf-bcf1-8222acca27cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xref",
   "language": "python",
   "name": "xref"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
